{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96bea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import dense_mincut_pool, ChebConv\n",
    "from scipy.linalg import sqrtm, inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd726ea",
   "metadata": {},
   "source": [
    "# Constructing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9111112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahman Khorramfar\\AppData\\Local\\Temp\\ipykernel_10712\\1922429778.py:20: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  solar = solar.loc[solar.index.date != pd.to_datetime('2016-02-29')]\n",
      "C:\\Users\\Rahman Khorramfar\\AppData\\Local\\Temp\\ipykernel_10712\\1922429778.py:27: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  wind = wind.loc[wind.index.date != pd.to_datetime('2016-02-29')]\n"
     ]
    }
   ],
   "source": [
    "coords = pd.read_csv('Data/Power Network Topology-full network (188 nodes)/Electricity Sytem Nodes-location.csv')\n",
    "coords_agg = coords.drop_duplicates(subset=['lat', 'lon']) # remove duplicate coordinates\n",
    "\n",
    "# read in electricity data\n",
    "elec = pd.read_csv('Data/Power Network Topology-full network (188 nodes)/bus_load_RM_2050.csv', index_col=[0])\n",
    "elec = pd.DataFrame(StandardScaler().fit_transform(elec)) # standardizing\n",
    "\n",
    "# create hourly datetime index\n",
    "times = pd.date_range(start='2050-01-01', end = '2051-01-01', freq='H')[:-1]\n",
    "elec.index = times\n",
    "\n",
    "# NG data\n",
    "ng = pd.read_csv('Data/ng_daily_load2050_RM.csv', index_col=[0])\n",
    "ng.index = pd.date_range(start='2050-01-01', end = '2050-12-31', freq='d')\n",
    "ng = pd.DataFrame(StandardScaler().fit_transform(ng)) # standardizing\n",
    "\n",
    "# solar data\n",
    "solar = pd.read_csv('Data/solar-CF-188-nodes.csv', index_col=[0])\n",
    "solar.index = pd.date_range(start='2016-01-01', end = '2017-01-01', freq='H')[:-1]\n",
    "solar = solar.loc[solar.index.date != pd.to_datetime('2016-02-29')]\n",
    "solar = pd.DataFrame(StandardScaler().fit_transform(solar)) # standardizing\n",
    "solar.index = times\n",
    "\n",
    "# wind data\n",
    "wind = pd.read_csv('Data/wind-CF-188-nodes.csv', index_col=[0])\n",
    "wind.index = pd.date_range(start='2016-01-01', end = '2017-01-01', freq='H')[:-1]\n",
    "wind = wind.loc[wind.index.date != pd.to_datetime('2016-02-29')]\n",
    "wind = pd.DataFrame(StandardScaler().fit_transform(wind)) # standardizing\n",
    "wind.index = times\n",
    "\n",
    "# remove columns for nodes at same coordinates\n",
    "elec = elec[coords_agg.bus_num]\n",
    "solar = solar[coords_agg.bus_num]\n",
    "wind = wind[coords_agg.bus_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76a0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_E = torch.zeros(365,len(elec.columns),24)\n",
    "X_S = torch.zeros(365,len(solar.columns),24)\n",
    "X_W = torch.zeros(365,len(wind.columns),24)\n",
    "X_NG = torch.tensor(ng.to_numpy(), dtype=torch.float)\n",
    "\n",
    "for date in range(365):\n",
    "    for hour in range(24):\n",
    "        for node in range(len(elec.columns)):\n",
    "            X_E[date,node,hour] = elec.to_numpy()[date*24+hour, node]\n",
    "            X_S[date,node,hour] = solar.to_numpy()[date*24+hour, node]\n",
    "            X_W[date,node,hour] = wind.to_numpy()[date*24+hour, node]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408af7a",
   "metadata": {},
   "source": [
    "# Spatial Aggregation Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356463e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'Data/Power Network Topology-full network (188 nodes)/'\n",
    "edges = pd.read_csv(src + 'Branches Between Electricity Nodes.csv')\n",
    "edges = edges.loc[edges['if this branch is existing'] == 1] # do not consider candidate branches\n",
    "\n",
    "# create power/NG network graph\n",
    "G_net = nx.Graph()\n",
    "for node in coords_agg.bus_num:\n",
    "    G_net.add_node(node, pos = coords_agg.loc[coords_agg.bus_num==node][['lat','lon']].values.flatten())\n",
    "\n",
    "for x in G_net.nodes:\n",
    "    for y in G_net.nodes:\n",
    "        x_lat, x_lon = G_net.nodes[x]['pos']\n",
    "        y_lat, y_lon = G_net.nodes[y]['pos']\n",
    "        df = edges.loc[(edges['from bus'].isin(coords.loc[(coords.lat==x_lat) & \n",
    "                                                          (coords.lon==x_lon)].bus_num)) &\n",
    "                       (edges['to bus'].isin(coords.loc[(coords.lat==y_lat) & \n",
    "                                                        (coords.lon==y_lon)].bus_num))]\n",
    "        if x == y:\n",
    "            G_net.add_edge(x, y, distance=0) # add self edge\n",
    "        elif len(df) > 0:\n",
    "            G_net.add_edge(x, y, distance=df['Distance (mile)'].mean()) # add edge with distance\n",
    "            \n",
    "edge_ix = torch.tensor(np.array(nx.convert_node_labels_to_integers(G_net.copy()).edges),dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113321e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize geospatial graph\n",
    "G_geo = nx.Graph()\n",
    "for node in coords_agg.bus_num:\n",
    "    G_geo.add_node(node, pos = coords_agg.loc[coords_agg.bus_num==node][['lat','lon']].values.flatten())\n",
    "\n",
    "# create list of distances\n",
    "distances = list()\n",
    "for x in G_geo.nodes:\n",
    "    for y in G_geo.nodes:\n",
    "        if x != y:\n",
    "            (x1, y1) = G_geo.nodes[x]['pos']\n",
    "            (x2, y2) = G_geo.nodes[y]['pos']\n",
    "            dist = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "            distances.append(dist)\n",
    "\n",
    "# normalize distances\n",
    "i = 0\n",
    "sigma = np.std(distances)\n",
    "for x in G_geo.nodes:\n",
    "    for y in G_geo.nodes:\n",
    "        if x != y:\n",
    "            G_geo.add_edge(x, y, distance=np.exp((-distances[i]**2) / (sigma**2)))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ecf9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_AE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, clusters):\n",
    "        super(Spatial_AE, self).__init__()\n",
    "\n",
    "        self.pool_conv1 = ChebConv(24 + len(G_net.nodes), clusters*3, K=1)\n",
    "        self.pool_conv2 = ChebConv(clusters*3, clusters*2, K=1)\n",
    "        self.pool_conv3 = Linear(clusters*2, clusters)\n",
    "\n",
    "        # Electricity Convolution Layers\n",
    "        self.conv1 = Linear(24, 18)\n",
    "        self.conv2 = Linear(18, 12)\n",
    "\n",
    "        encoder_params=0\n",
    "        for p in list(self.parameters()):\n",
    "            n=1\n",
    "            for s in list(p.size()):\n",
    "                n = n*s\n",
    "            encoder_params += n\n",
    "        print(\"Encoder Parameters: \" + str(encoder_params))\n",
    "\n",
    "        # Electricity Block Decoder Parameters\n",
    "        self.deconv3 = Linear(12, 18)\n",
    "        self.deconv4 = Linear(18, 24)\n",
    "\n",
    "        decoder_params = 0\n",
    "        for p in list(self.parameters()):\n",
    "            n=1\n",
    "            for s in list(p.size()):\n",
    "                n = n*s\n",
    "            decoder_params += n\n",
    "        print(\"Decoder Parameters: \" + str(decoder_params - encoder_params))\n",
    "\n",
    "        print(\"Total Parameters: \" + str(decoder_params))\n",
    "\n",
    "    def forward(self, X, edge_ix, A_net, A_geo):\n",
    "\n",
    "        X_mean = X.mean(dim=1, keepdim=True)\n",
    "        X_norm = X - X_mean\n",
    "        X_norm = torch.cat((X_norm, \n",
    "                            torch.eye(X_norm.shape[1]).repeat([X_norm.shape[0],1,1])),\n",
    "                            dim=-1)\n",
    "\n",
    "        X = self.conv1(X).tanh()\n",
    "        X = self.conv2(X)\n",
    "\n",
    "        S = self.pool_conv1(X_norm, edge_ix).relu()\n",
    "        S = self.pool_conv2(S, edge_ix).relu()\n",
    "        S = self.pool_conv3(S)\n",
    "\n",
    "        _, _, Lc_geo, Lo_geo = dense_mincut_pool(X, A_geo, S)\n",
    "        encoded, A_pool, Lc_net, Lo_net = dense_mincut_pool(X, A_net, S)\n",
    "\n",
    "        X_rec = torch.matmul(S.softmax(dim=-1), encoded)\n",
    "\n",
    "        X_rec = self.deconv3(X_rec).tanh()\n",
    "        X_rec = self.deconv4(X_rec)\n",
    "        decoded = X_rec.squeeze(0)\n",
    "\n",
    "        return encoded, decoded, S.softmax(dim=-1), Lc_net, Lo_net, Lc_geo, Lo_geo, A_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab2bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation data\n",
    "X_E_train, X_E_val = train_test_split(X_E, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalize adjacency matrices\n",
    "A_net = torch.tensor(nx.to_numpy_matrix(G_net), dtype=torch.float)\n",
    "D_net = torch.tensor(np.diag(A_net.sum(axis=1)))\n",
    "A_net = torch.tensor(inv(sqrtm(D_net)), dtype=torch.float) @ A_net @ torch.tensor(inv(sqrtm(D_net)), dtype=torch.float)\n",
    "\n",
    "A_geo = torch.tensor(nx.to_numpy_matrix(G_geo, weight='distance'), dtype=torch.float)\n",
    "D_geo = torch.tensor(np.diag(A_geo.sum(axis=1)))\n",
    "A_geo = torch.tensor(inv(sqrtm(D_geo)), dtype=torch.float) @ A_geo @ torch.tensor(inv(sqrtm(D_geo)), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51496bd9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ceab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Parameters: 3018\n",
      "Decoder Parameters: 690\n",
      "Total Parameters: 3708\n"
     ]
    }
   ],
   "source": [
    "clusters = 6 # choose number of spatial clusters\n",
    "\n",
    "model = Spatial_AE(clusters)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "410d8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "train = list()\n",
    "val = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127a0f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.05319987237453461: 100%|████████████████████████████████████████| 1000/1000 [00:54<00:00, 18.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=epochs, desc='Progress', disable=False) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        encoded, decoded, S, Lc_net, Lo_net, Lc_geo, Lo_geo,_ = model(X_E_train, edge_ix, A_net, A_geo)\n",
    "        loss = criterion(decoded, X_E_train)\n",
    "        train.append(loss.item())\n",
    "        loss = loss + 0.5 * (Lc_net + Lo_net) + 0.5 * (Lc_geo + Lo_geo)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        encoded, decoded, S, L_s, L_o, Ls_geo, Lo_geo,_ = model(X_E_val, edge_ix, A_net, A_geo)\n",
    "\n",
    "        val_loss = criterion(decoded, X_E_val)\n",
    "        val.append(val_loss.item())\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"Validation Loss: \" + str(val[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f06cca",
   "metadata": {},
   "source": [
    "## Saving Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "115dbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, S, _, _, _, _, _ = model(X_E, edge_ix, A_net, A_geo)\n",
    "C = S.mean(dim=0).argmax(dim=1).detach().numpy()\n",
    "C = pd.DataFrame([list(range(len(G_net.nodes))), C]).astype(int).T\n",
    "C.columns = ['Node', 'Cluster']\n",
    "rows = list()\n",
    "for i in range(len(G_net.nodes)):\n",
    "    assignment = C.iloc[i]['Cluster']\n",
    "    for node in G_net.nodes:\n",
    "        rows.append([node, assignment])\n",
    "C = pd.DataFrame(rows)\n",
    "C.columns = ['Node', 'Cluster']\n",
    "\n",
    "C.to_csv(f'spatial_cluster.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f95a5",
   "metadata": {},
   "source": [
    "# Temporal Aggregation Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc9e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment power graph to create joint power/NG graph\n",
    "NG_coords = pd.read_csv('Data/NG Network Topology/new_england_ng_nodes.csv')[['Lat','Lon']].to_numpy()\n",
    "NG_edges = pd.read_csv('Data/Power Network Topology-full network (188 nodes)/NG_adj-Eelectricity nodes-188-nodes.csv')\n",
    "NG_edges['NG Node'] += len(elec.columns)\n",
    "for i in range(len(NG_edges['NG Node'])):\n",
    "    G_net.add_node(NG_edges['NG Node'][i], pos=NG_coords[i])\n",
    "\n",
    "for node in NG_edges['NG Node']:\n",
    "    G_net.add_edge(node, node, distance=0)\n",
    "\n",
    "for ix, row in NG_edges.iterrows():\n",
    "    row = row.dropna().astype(int).tolist()\n",
    "    for node in row[1:]:\n",
    "        x,y = coords.loc[coords.bus_num == node][['lat','lon']].values.flatten()\n",
    "        agg_node = coords.loc[(coords.lat == x) & (coords.lon == y)].bus_num.values.flatten()[0]\n",
    "        G_net.add_edge(row[0], agg_node, distance=float(np.mean(distances)))\n",
    "\n",
    "NG_edges = pd.read_csv('Data/NG Network Topology/new_england_ng_lines - unidirectional.csv')\n",
    "\n",
    "NG_edges['from_node'] = NG_edges['from_node'].astype(int) + len(elec.columns)\n",
    "NG_edges['to_node'] = NG_edges['to_node'].astype(int) + len(elec.columns)\n",
    "G_net.add_edges_from(NG_edges[['from_node', 'to_node']].to_numpy())\n",
    "\n",
    "NG_edge_index = torch.tensor(NG_edges[['from_node', 'to_node']].to_numpy(),dtype=torch.long).t().contiguous()\n",
    "NG_edge_index -= len(elec.columns)\n",
    "\n",
    "edge_ix = torch.tensor(np.array(nx.convert_node_labels_to_integers(G_net.copy()).edges),dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee73f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_AE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Temporal_AE, self).__init__()\n",
    "\n",
    "        # Joint Convolutions\n",
    "        self.encoder_joint_1 = ChebConv(73, 48, K=1)\n",
    "        self.encoder_joint_2 = ChebConv(48, 3, K=1)\n",
    "\n",
    "        encoder_params=0\n",
    "        for p in list(self.parameters()):\n",
    "            n=1\n",
    "            for s in list(p.size()):\n",
    "                n = n*s\n",
    "            encoder_params += n\n",
    "        print(\"Encoder Parameters: \" + str(encoder_params))\n",
    "\n",
    "        self.decoder_joint_1 = ChebConv(3, 73, K=1)\n",
    "\n",
    "        # Electricity Block Decoder Parameters\n",
    "        self.decoder_E1 = Linear(73, 73)\n",
    "        self.decoder_E2 = Linear(73, 72)\n",
    "\n",
    "        # Natural Gas Block Encoder Parameters\n",
    "        self.decoder_NG1 = nn.Linear(73, 73)\n",
    "        self.decoder_NG2 = nn.Linear(73, 1)\n",
    "\n",
    "        decoder_params = 0\n",
    "        for p in list(self.parameters()):\n",
    "            n=1\n",
    "            for s in list(p.size()):\n",
    "                n = n*s\n",
    "            decoder_params += n\n",
    "        print(\"Decoder Parameters: \" + str(decoder_params - encoder_params))\n",
    "\n",
    "        print(\"Total Parameters: \" + str(decoder_params))\n",
    "\n",
    "    def forward(self, X_E, X_NG, edge_ix, return_cluster=False):\n",
    "\n",
    "        X_NG = X_NG.reshape(-1,18,1)\n",
    "        X = torch.cat([torch.block_diag(X_E[i], X_NG[i]).unsqueeze(0) for i in range(X_E.shape[0])])\n",
    "        X = self.encoder_joint_1(X, edge_ix).tanh()\n",
    "        X = self.encoder_joint_2(X, edge_ix)\n",
    "        encoded = X.reshape(-1,106,3)\n",
    "\n",
    "        X = self.decoder_joint_1(encoded, edge_ix)\n",
    "        X_E, X_NG = torch.split(X, [88, 18], dim=1)\n",
    "\n",
    "        X_E = X_E.reshape(-1,88,73)\n",
    "        X_E = self.decoder_E1(X_E).tanh()\n",
    "        X_E = self.decoder_E2(X_E)\n",
    "\n",
    "        # natural gas decoder\n",
    "        X_NG = X_NG.reshape(-1,18,73)\n",
    "        X_NG = self.decoder_NG1(X_NG).tanh()\n",
    "        X_NG = self.decoder_NG2(X_NG).reshape(-1,18)\n",
    "       \n",
    "        return encoded, X_E, X_NG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d08fb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate electrical, wind, solar data\n",
    "X = torch.cat([X_E, X_W, X_S], axis=2)\n",
    "\n",
    "# split train and validation data\n",
    "X_train, X_val, X_NG_train, X_NG_val = train_test_split(X, X_NG, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926fe30",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e684ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Parameters: 3699\n",
      "Decoder Parameters: 16498\n",
      "Total Parameters: 20197\n"
     ]
    }
   ],
   "source": [
    "days = 7 # set number of representative days\n",
    "\n",
    "model = Temporal_AE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ceb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_G = 2\n",
    "alpha_W = 0.5\n",
    "alpha_S = 0.5\n",
    "\n",
    "epochs = 1000\n",
    "train = list()\n",
    "val = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2bf0d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.23755723237991333: 100%|████████████████████████████████████████| 1000/1000 [01:42<00:00,  9.78it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=epochs, desc='Progress', disable=False) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        X_encoded, X_decoded, X_NG_decoded = model(X_train, X_NG_train, edge_ix)\n",
    "        loss = criterion(X_decoded[:,:,:24], X_train[:,:,:24]) + \\\n",
    "               alpha_G*criterion(X_NG_decoded, X_NG_train) + \\\n",
    "               alpha_W*criterion(X_decoded[:,:,24:48], X_train[:,:,24:48]) + \\\n",
    "               alpha_S*criterion(X_decoded[:,:,48:], X_train[:,:,48:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train.append(loss.item())\n",
    "\n",
    "        X_encoded, X_decoded, X_NG_decoded = model(X_val, X_NG_val, edge_ix)\n",
    "        val_loss = criterion(X_decoded[:,:,:24], X_val[:,:,:24]) + \\\n",
    "               alpha_G*criterion(X_NG_decoded, X_NG_val) + \\\n",
    "               alpha_W*criterion(X_decoded[:,:,24:48], X_val[:,:,24:48]) + \\\n",
    "               alpha_S*criterion(X_decoded[:,:,48:], X_val[:,:,48:])\n",
    "        val.append(val_loss.item())\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"Validation Loss: \" + str(val[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa123a",
   "metadata": {},
   "source": [
    "## Saving Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04ae48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve latent embeddings from trained model\n",
    "X_latent, _, _ = model(X, X_NG, edge_ix)\n",
    "X_latent = X_latent.detach().numpy().reshape(365,-1)\n",
    "\n",
    "# fit K-medoids\n",
    "kmedoids = KMedoids(n_clusters=days, init='k-medoids++', max_iter=5000).fit(X_latent)\n",
    "cluster_dates = np.unique(elec.index.date)[kmedoids.medoid_indices_]\n",
    "row_list = list()\n",
    "\n",
    "# create csv for learned cluster medians\n",
    "for i in range(days):\n",
    "    row = pd.DataFrame([list(np.unique(elec.index.date)).index(cluster_dates[i]), \n",
    "                        cluster_dates[i], \n",
    "                        sum(kmedoids.labels_ == i)]).T\n",
    "    row_list.append(row)\n",
    "\n",
    "kmedoids_results = pd.concat(row_list)\n",
    "kmedoids_results.columns = ['Day of Year', 'Date', 'Weight']\n",
    "kmedoids_results = kmedoids_results.sort_values(by='Day of Year')\n",
    "kmedoids_results.to_csv('temporal_cluster.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5316f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
